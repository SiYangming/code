---
title: "Otto数据集"
output:
  html_document:
    df_print: paged
    toc: true
    theme: united
---

# 1.并行化训练模

```{python}
# Otto, tune number of threads
from pandas import read_csv
from xgboost import XGBClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from time import time
from matplotlib import pyplot 
# load data  
data = read_csv('train.csv')
dataset = data.values
# split data into X and y
X = dataset[:,0:94]
y = dataset[:,94]
# encode string class values as integers
label_encoded_y = LabelEncoder().fit_transform(y)
# split data into train and test sets
seed = 7
test_size = 0.77
X_train, X_test, y_train, y_test = train_test_split(X, label_encoded_y, test_size=test_size,
    random_state=seed)
# evaluate the effect of the number of threads
results = []
num_threads = [1, 2, 3, 4]
for n in num_threads:
    start = time()
    model = XGBClassifier(nthread=n)
    model.fit(X_train, y_train)
    elapsed = time() - start
    print(n, elapsed)
    results.append(elapsed)
```

```{python}
# plot results
pyplot.clf()
pyplot.plot(num_threads, results)  
pyplot.ylabel('Speed (seconds)')  
pyplot.xlabel('Number of Threads')  
pyplot.title('XGBoost Training Speed vs Number of Threads') 
pyplot.show()
```


```{python}
# prepare cross validation  
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)  
```

```{python}
# Single Thread XGBoost, Parallel Thread CV  
# Parallel Thread XGBoost, Single Thread CV  
start = time()
model = XGBClassifier(nthread=-1)
results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_log_loss', n_jobs=1)
elapsed = time() - start
print("Single Thread XGBoost, Parallel Thread CV: %f" % (elapsed))
print("Parallel Thread XGBoost, Single Thread CV: %f" % (elapsed))
```

```{python}
# Parallel Thread XGBoost and CV  
start = time()
model = XGBClassifier(nthread=-1)
results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_log_loss', n_jobs=-1)
elapsed = time() - start
print("Parallel Thread XGBoost and CV: %f" % (elapsed))
```

# 2.XGboost调参

## 2.1.调整树的数量

```{python}
# XGBoost on Otto dataset, Tune n_estimators
from sklearn.model_selection import GridSearchCV  
from sklearn.model_selection import StratifiedKFold  
import matplotlib  
matplotlib.use('Agg')  
```

```{python}
# grid search  
model = XGBClassifier()  
n_estimators = range(50, 400, 50)  
param_grid = dict(n_estimators=n_estimators)  
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)  
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold) 
grid_result = grid_search.fit(X_train, y_train)  
# summarize results  
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
```


```{python}
means = grid_result.cv_results_['mean_test_score']  
stds = grid_result.cv_results_['std_test_score']  
params = grid_result.cv_results_['params']  
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
```

```{python}
# plot
pyplot.clf()
pyplot.errorbar(n_estimators, means, yerr=stds) 
pyplot.title("XGBoost n_estimators vs Log Loss")
pyplot.xlabel('n_estimators')  
pyplot.ylabel('Log Loss') 
pyplot.savefig('n_estimators.png')
```

## 2.2.调整决策树的大小

```{python}
# XGBoost on Otto dataset, Tune max_depth
matplotlib.use('Agg')  
# grid search  
max_depth = range(1, 11, 2)  
print(max_depth)  
```

```{python}
param_grid = dict(max_depth=max_depth)  
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)  
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold, verbose=1)  
grid_result = grid_search.fit(X_train, y_train) 
# summarize results  
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
```

```{python}
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
```

```{python}
# plot
pyplot.clf()
pyplot.errorbar(max_depth, means, yerr=stds)
pyplot.title("XGBoost max_depth vs Log Loss")
pyplot.xlabel('max_depth')
pyplot.ylabel('Log Loss')
pyplot.savefig('max_depth.png')
```

## 2.3.学习率

```{python}
# XGBoost on Otto dataset, Tune learning_rate
from pandas import read_csv  
from xgboost import XGBClassifier  
from sklearn.model_selection import GridSearchCV  
from sklearn.model_selection import StratifiedKFold  
from sklearn.preprocessing import LabelEncoder  
import matplotlib  
matplotlib.use('Agg')  
from matplotlib import pyplot  

# grid search  
model = XGBClassifier()  
learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]  
param_grid = dict(learning_rate=learning_rate)  
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)  
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X_train, y_train)
# summarize results  
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_)) 
```

```{python}
means = grid_result.cv_results_['mean_test_score']  
stds = grid_result.cv_results_['std_test_score']  
params = grid_result.cv_results_['params']  
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
```

```{python}
# plot
pyplot.clf()
pyplot.errorbar(learning_rate, means, yerr=stds)
pyplot.title("XGBoost learning_rate vs Log Loss") 
pyplot.xlabel('learning_rate')  
pyplot.ylabel('Log Loss') 
pyplot.savefig('learning_rate.png')
```

## 2.4.树的数量和学习率

```{python}
# XGBoost on Otto dataset, Tune learning_rate and n_estimators
from pandas import read_csv  
from xgboost import XGBClassifier  
from sklearn.model_selection import GridSearchCV  
from sklearn.model_selection import StratifiedKFold  
from sklearn.preprocessing import LabelEncoder  
import matplotlib  
matplotlib.use('Agg')  
from matplotlib import pyplot  
import numpy  

# grid search  
model = XGBClassifier()  
n_estimators = [100, 200, 300, 400, 500]  
learning_rate = [0.0001, 0.001, 0.01, 0.1]  
param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)  
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)  
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X_train, y_train)  
# summarize results  
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
```

```{python}
means = grid_result.cv_results_['mean_test_score']  
stds = grid_result.cv_results_['std_test_score']  
params = grid_result.cv_results_['params']  
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
```

```{python}
# plot results
scores = numpy.array(means).reshape(len(learning_rate), len(n_estimators))
pyplot.clf()
for i, value in enumerate(learning_rate):
    pyplot.plot(n_estimators, scores[i], label='learning_rate: ' + str(value))
```

```{python}
pyplot.legend()  
pyplot.xlabel('n_estimators')
pyplot.ylabel('Log Loss')
pyplot.savefig('n_estimators_vs_learning_rate.png')
```

## 2.5.Subsample

```{python}
# XGBoost on Otto dataset, tune subsample
# grid search  
model = XGBClassifier()  
subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]  
param_grid = dict(subsample=subsample)  
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X_train, y_train)  
# summarize results  
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
```

```{python}
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
```

```{python}
# plot
pyplot.clf()
pyplot.errorbar(subsample, means, yerr=stds)
pyplot.title("XGBoost subsample vs Log Loss")
pyplot.xlabel('subsample')
pyplot.ylabel('Log Loss')
pyplot.savefig('subsample.png')
```

### 2.5.1.Subsample By Tree

```{python}
# XGBoost on Otto dataset, tune colsample_bytree
# grid search  
model = XGBClassifier()  
colsample_bytree = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]  
param_grid = dict(colsample_bytree=colsample_bytree)  
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)  
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X_train, y_train)  
# summarize results  
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))  
```

```{python}
means = grid_result.cv_results_['mean_test_score']  
stds = grid_result.cv_results_['std_test_score']  
params = grid_result.cv_results_['params']  
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
```

```{python}
# plot
pyplot.clf()
pyplot.errorbar(colsample_bytree, means, yerr=stds) 
pyplot.title("XGBoost colsample_bytree vs Log Loss") 
pyplot.xlabel('colsample_bytree')  
pyplot.ylabel('Log Loss')
pyplot.savefig('colsample_bytree.png')
```

### 2.5.2.Subsample By Split

```{python}
# XGBoost on Otto dataset, tune colsample_bylevel
from pandas import read_csv  
from xgboost import XGBClassifier  
from sklearn.model_selection import GridSearchCV 
from sklearn.model_selection import StratifiedKFold 
from sklearn.preprocessing import LabelEncoder 
import matplotlib  
matplotlib.use('Agg')  
from matplotlib import pyplot  

# grid search
model = XGBClassifier()  
colsample_bylevel = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]  
param_grid = dict(colsample_bylevel=colsample_bylevel)  
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)  
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X_train, y_train)  
# summarize results  
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_)) 
```

```{python}
means = grid_result.cv_results_['mean_test_score']  
stds = grid_result.cv_results_['std_test_score']  
params = grid_result.cv_results_['params']  
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
```

```{python}
# plot
pyplot.clf()
pyplot.errorbar(colsample_bylevel, means, yerr=stds) 
pyplot.title("XGBoost colsample_bylevel vs Log Loss") 
pyplot.xlabel('colsample_bylevel')  
pyplot.ylabel('Log Loss')
pyplot.savefig('colsample_bylevel.png')
```
